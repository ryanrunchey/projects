{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12d4001-2b71-496a-8d49-eceb261a84da",
   "metadata": {},
   "source": [
    "# Historical Stock Price Data\n",
    "\n",
    "### Polygon: https://polygon.io/dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "839849a0-c37e-4626-b837-fb293c390679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your Polygon.io API Key\n",
    "API_KEY = \"TlQH6pl7Yc8D7E9peGk2IXF0EsBTNGxi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a9dbb8c-86a5-40a3-ad1d-c6cf7ef71352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pandas_market_calendars as mcal\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm  # Use tqdm.auto if not in notebook\n",
    "from time import sleep\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec790f9c-378f-467a-86c0-025ba1e62b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google BigQuery Authentication\n",
    "from google.cloud import bigquery\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "# Export data\n",
    "from pandas_gbq import to_gbq\n",
    "import pickle\n",
    "\n",
    "# Set your OAuth client ID JSON downloaded from GCP Console\n",
    "# CLIENT_SECRET_FILE = 'client_secret.json'  # downloaded from GCP\n",
    "CLIENT_SECRET_FILE = os.path.expanduser(\"/Users/ryanrunchey/credentials/gcp_credentials/client_secret_295707256455-0fsr3bqoc89psl22fgp2cfipbd4m1s1v.apps.googleusercontent.com.json\")\n",
    "SCOPES = ['https://www.googleapis.com/auth/cloud-platform']\n",
    "\n",
    "# Authenticate interactively (stores a token locally for reuse)\n",
    "if os.path.exists('token.pickle'):\n",
    "    with open('token.pickle', 'rb') as token:\n",
    "        credentials = pickle.load(token)\n",
    "else:\n",
    "    flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRET_FILE, SCOPES)\n",
    "    credentials = flow.run_local_server(port=0)\n",
    "    with open('token.pickle', 'wb') as token:\n",
    "        pickle.dump(credentials, token)\n",
    "\n",
    "# Initialize the BigQuery client with those credentials\n",
    "client = bigquery.Client(credentials=credentials, project=\"ryanrunchey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cad08bb-41e8-4a5d-8ea1-da988fa38f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002-09-18</td>\n",
       "      <td>2005-01-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-09-17</td>\n",
       "      <td>2006-01-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-09-15</td>\n",
       "      <td>2007-01-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-09-16</td>\n",
       "      <td>2008-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-09-15</td>\n",
       "      <td>2009-01-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2007-09-14</td>\n",
       "      <td>2010-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2008-09-17</td>\n",
       "      <td>2011-01-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2009-09-16</td>\n",
       "      <td>2012-01-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2010-09-15</td>\n",
       "      <td>2013-01-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2011-09-16</td>\n",
       "      <td>2014-01-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2012-09-14</td>\n",
       "      <td>2015-01-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2013-09-18</td>\n",
       "      <td>2016-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2014-09-17</td>\n",
       "      <td>2017-01-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2015-09-16</td>\n",
       "      <td>2018-01-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2016-09-16</td>\n",
       "      <td>2019-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017-09-15</td>\n",
       "      <td>2020-01-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-09-14</td>\n",
       "      <td>2021-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-09-18</td>\n",
       "      <td>2022-01-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>2023-01-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-09-16</td>\n",
       "      <td>2024-01-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    start_date    end_date\n",
       "0   2002-09-18  2005-01-18\n",
       "1   2003-09-17  2006-01-17\n",
       "2   2004-09-15  2007-01-16\n",
       "3   2005-09-16  2008-01-15\n",
       "4   2006-09-15  2009-01-13\n",
       "5   2007-09-14  2010-01-12\n",
       "6   2008-09-17  2011-01-18\n",
       "7   2009-09-16  2012-01-17\n",
       "8   2010-09-15  2013-01-14\n",
       "9   2011-09-16  2014-01-14\n",
       "10  2012-09-14  2015-01-13\n",
       "11  2013-09-18  2016-01-19\n",
       "12  2014-09-17  2017-01-17\n",
       "13  2015-09-16  2018-01-16\n",
       "14  2016-09-16  2019-01-15\n",
       "15  2017-09-15  2020-01-14\n",
       "16  2018-09-14  2021-01-12\n",
       "17  2019-09-18  2022-01-18\n",
       "18  2020-09-16  2023-01-17\n",
       "19  2021-09-16  2024-01-16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start dates\n",
    "start_dates = [\n",
    "    '2002-09-18', '2003-09-17', '2004-09-15', '2005-09-16', '2006-09-15',\n",
    "    '2007-09-14', '2008-09-17', '2009-09-16', '2010-09-15', '2011-09-16',\n",
    "    '2012-09-14', '2013-09-18', '2014-09-17', '2015-09-16', '2016-09-16',\n",
    "    '2017-09-15', '2018-09-14', '2019-09-18', '2020-09-16', '2021-09-16'\n",
    "    # '2022-09-14'  # Optional: add if needed\n",
    "]\n",
    "\n",
    "# NYSE trading calendar\n",
    "nyse = mcal.get_calendar('NYSE')\n",
    "\n",
    "# Compute end_dates: first NYSE trading day ≥ start_date + 851 days\n",
    "start_date_objs = [datetime.strptime(d, \"%Y-%m-%d\") for d in start_dates]\n",
    "end_dates = []\n",
    "\n",
    "for start_dt in start_date_objs:\n",
    "    target_dt = start_dt + timedelta(days=851)\n",
    "    sched = nyse.schedule(start_date=target_dt, end_date=target_dt + timedelta(days=20))\n",
    "    first_trading_day = sched.index[0].strftime(\"%Y-%m-%d\")\n",
    "    end_dates.append(first_trading_day)\n",
    "\n",
    "# Create dataframe\n",
    "df_ranges = pd.DataFrame({\n",
    "    \"start_date\": start_dates,\n",
    "    \"end_date\": end_dates\n",
    "})\n",
    "\n",
    "df_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6a97d37-50b7-4c40-b9bd-69809d72bfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4739.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Export to csv\n",
    "df_ranges.to_csv(\"historical_stock_dates.csv\")\n",
    "\n",
    "# Export to pickle\n",
    "df_ranges.to_pickle(\"historical_stock_dates.pkl\")\n",
    "\n",
    "# Export to BigQuery\n",
    "to_gbq(\n",
    "    dataframe=df_ranges,\n",
    "    destination_table=\"historical_stock_price_returns.historical_stock_dates\",\n",
    "    project_id=\"ryanrunchey\",\n",
    "    if_exists=\"replace\"  # or \"append\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91ad41d2-eb8e-4a6f-bd32-38cb0c49ec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2002-09-18', '2003-09-17', '2004-09-15', '2005-01-18', '2005-09-16', '2006-01-17', '2006-09-15', '2007-01-16', '2007-09-14', '2008-01-15', '2008-09-17', '2009-01-13', '2009-09-16', '2010-01-12', '2010-09-15', '2011-01-18', '2011-09-16', '2012-01-17', '2012-09-14', '2013-01-14', '2013-09-18', '2014-01-14', '2014-09-17', '2015-01-13', '2015-09-16', '2016-01-19', '2016-09-16', '2017-01-17', '2017-09-15', '2018-01-16', '2018-09-14', '2019-01-15', '2019-09-18', '2020-01-14', '2020-09-16', '2021-01-12', '2021-09-16', '2022-01-18', '2023-01-17', '2024-01-16']\n"
     ]
    }
   ],
   "source": [
    "# Create the list of dates\n",
    "\n",
    "# Combine, flatten, deduplicate, and sort\n",
    "dates = sorted(set(df_ranges['start_date']).union(df_ranges['end_date']))\n",
    "\n",
    "# Preview\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1803dd9-1254-4d6c-a179-58264270852c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing date: 2002-09-18\n",
      "Processing date: 2003-09-17\n",
      "Processing date: 2004-09-15\n",
      "Processing date: 2005-01-18\n",
      "Processing date: 2005-09-16\n",
      "Processing date: 2006-01-17\n",
      "Processing date: 2006-09-15\n",
      "Processing date: 2007-01-16\n",
      "Processing date: 2007-09-14\n",
      "Processing date: 2008-01-15\n",
      "Processing date: 2008-09-17\n",
      "Processing date: 2009-01-13\n",
      "Processing date: 2009-09-16\n",
      "Processing date: 2010-01-12\n",
      "Processing date: 2010-09-15\n",
      "Processing date: 2011-01-18\n",
      "Processing date: 2011-09-16\n",
      "Processing date: 2012-01-17\n",
      "Processing date: 2012-09-14\n",
      "Processing date: 2013-01-14\n",
      "Processing date: 2013-09-18\n",
      "Processing date: 2014-01-14\n",
      "Processing date: 2014-09-17\n",
      "Processing date: 2015-01-13\n",
      "Processing date: 2015-09-16\n",
      "Processing date: 2016-01-19\n",
      "Processing date: 2016-09-16\n",
      "Processing date: 2017-01-17\n",
      "Processing date: 2017-09-15\n",
      "Processing date: 2018-01-16\n",
      "Processing date: 2018-09-14\n",
      "Processing date: 2019-01-15\n",
      "Processing date: 2019-09-18\n",
      "Processing date: 2020-01-14\n",
      "Processing date: 2020-09-16\n",
      "Processing date: 2021-01-12\n",
      "Processing date: 2021-09-16\n",
      "Processing date: 2022-01-18\n",
      "Processing date: 2023-01-17\n",
      "Processing date: 2024-01-16\n",
      "✅ Saved: all_ticker_exchange_mappings.csv\n"
     ]
    }
   ],
   "source": [
    "# Ticker to exchange mapping\n",
    "exchanges = [\"XASE\", \"XNAS\"] # 'AMEX':'XASE, 'NASDAQ':'XNAS', 'NYSE': 'XNYS'\n",
    "\n",
    "all_mappings = []\n",
    "\n",
    "def get_exchange_tickers_for_date(exchange, date):\n",
    "    tickers = []\n",
    "    url = f\"https://api.polygon.io/v3/reference/tickers?market=stocks&exchange={exchange}&active=true&date={date}&limit=1000&apiKey={API_KEY}\"\n",
    "    \n",
    "    while url:\n",
    "        r = requests.get(url)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        for item in data.get(\"results\", []):\n",
    "            tickers.append({\n",
    "                \"ticker\": item[\"ticker\"],\n",
    "                \"exchange\": exchange,\n",
    "                \"date\": date\n",
    "            })\n",
    "        url = data.get(\"next_url\")\n",
    "        if url:\n",
    "            url += f\"&apiKey={API_KEY}\"\n",
    "        # sleep(0.1)  # Optional: use if rate limits encountered\n",
    "\n",
    "    return tickers\n",
    "\n",
    "# Build the full mapping\n",
    "for date in dates:\n",
    "    print(f\"Processing date: {date}\")\n",
    "    for exch in exchanges:\n",
    "        tickers = get_exchange_tickers_for_date(exch, date)\n",
    "        all_mappings.extend(tickers)\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df = pd.DataFrame(all_mappings)\n",
    "df.to_csv(\"all_ticker_exchange_mappings.csv\", index=False)\n",
    "print(\"✅ Saved: all_ticker_exchange_mappings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0450d2be-b690-459c-aa32-71198c2bb0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full ticker-to-exchange map with dates\n",
    "df_map = pd.read_csv(\"all_ticker_exchange_mappings.csv\")\n",
    "\n",
    "# Keep only desired exchanges\n",
    "# exchanges = [\"XNYS\", \"XASE\"]\n",
    "df_map = df_map[df_map[\"exchange\"].isin(exchanges)]\n",
    "\n",
    "# Create a dictionary: {date: set of valid tickers}\n",
    "date_ticker_map = (\n",
    "    df_map.groupby(\"date\")[\"ticker\"]\n",
    "    .apply(set)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# List of dates to fetch\n",
    "DATES = sorted(date_ticker_map.keys())\n",
    "\n",
    "# Fetch grouped EOD data for a given date\n",
    "def fetch_grouped_for_date(date):\n",
    "    url = f\"https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/{date}?adjusted=true&apiKey={API_KEY}\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        results = data.get(\"results\", [])\n",
    "        rows = []\n",
    "\n",
    "        valid_tickers = date_ticker_map.get(date, set())\n",
    "        exchange_lookup = df_map[df_map[\"date\"] == date].set_index(\"ticker\")[\"exchange\"].to_dict()\n",
    "\n",
    "        for item in results:\n",
    "            ticker = item[\"T\"]\n",
    "            if ticker in valid_tickers:\n",
    "                rows.append({\n",
    "                    \"exchange\": exchange_lookup[ticker],\n",
    "                    \"ticker\": ticker,\n",
    "                    \"date\": date,\n",
    "                    \"close\": item.get(\"c\")\n",
    "                })\n",
    "\n",
    "        return rows\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {date}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Fetch all grouped data in parallel by date\n",
    "def fetch_all_grouped_data(dates, max_workers=16):\n",
    "    all_data = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(fetch_grouped_for_date, d): d for d in dates}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Fetching grouped data\"):\n",
    "            rows = future.result()\n",
    "            all_data.extend(rows)\n",
    "    return all_data\n",
    "\n",
    "# Final pipeline runner\n",
    "def run_grouped_pipeline():\n",
    "    print(\"Fetching grouped EOD data...\")\n",
    "    data = fetch_all_grouped_data(DATES)\n",
    "    df = pd.DataFrame(data)\n",
    "    if df.empty:\n",
    "        print(\"No data returned.\")\n",
    "        return df\n",
    "    return df[['exchange', 'ticker', 'date', 'close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05c49ed7-2bdf-4e66-9180-9df13f3a33ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching grouped EOD data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd0dfa527b44c1686e194cbcc9ac7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching grouped data:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exchange</th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XASE</td>\n",
       "      <td>KRY</td>\n",
       "      <td>2006-09-15</td>\n",
       "      <td>3.1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XASE</td>\n",
       "      <td>VUG</td>\n",
       "      <td>2006-09-15</td>\n",
       "      <td>54.2900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XASE</td>\n",
       "      <td>BHM</td>\n",
       "      <td>2006-09-15</td>\n",
       "      <td>10.9005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XASE</td>\n",
       "      <td>BGF</td>\n",
       "      <td>2006-09-15</td>\n",
       "      <td>18.9500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XASE</td>\n",
       "      <td>EIF</td>\n",
       "      <td>2006-09-15</td>\n",
       "      <td>14.2400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  exchange ticker        date    close\n",
       "0     XASE    KRY  2006-09-15   3.1300\n",
       "1     XASE    VUG  2006-09-15  54.2900\n",
       "2     XASE    BHM  2006-09-15  10.9005\n",
       "3     XASE    BGF  2006-09-15  18.9500\n",
       "4     XASE    EIF  2006-09-15  14.2400"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = run_grouped_pipeline()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea5cbb4f-edb4-4495-a1b3-9dc571a95ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 85859 entries, 0 to 85858\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   exchange  85859 non-null  object \n",
      " 1   ticker    85859 non-null  object \n",
      " 2   date      85859 non-null  object \n",
      " 3   close     85859 non-null  float64\n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a85d0193-5992-4dbe-b904-f9b5fe5833e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2003-09-17', '2024-01-16')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['date'].min(), df['date'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfc7f3c-c55f-4bb5-9241-0f142566979b",
   "metadata": {},
   "source": [
    "# Write Data to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49db4b73-7a24-4542-981d-42ec6d481d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6615.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# Export to csv\n",
    "df.to_csv(\"polygon_historical_stock_prices.csv\")\n",
    "\n",
    "# Export to pickle\n",
    "df.to_pickle(\"polygon_historical_stock_prices.pkl\")\n",
    "\n",
    "# Export to BigQuery\n",
    "to_gbq(\n",
    "    dataframe=df,\n",
    "    destination_table=\"historical_stock_price_returns.polygon_historical_stock_prices\",\n",
    "    project_id=\"ryanrunchey\",\n",
    "    if_exists=\"replace\"  # or \"append\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3858a30f-a834-4564-a575-ef2fe645b60d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
