{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../../data_PowerLaws_ForecastingEnergyConsumption/'\n",
    "\n",
    "df_holidays = pd.read_csv(directory + 'holidays.csv', parse_dates=['Date'])\n",
    "df_metadata = pd.read_csv(directory + 'metadata.csv')\n",
    "df_submission_format = pd.read_csv(directory + 'submission_format.csv', parse_dates=['Timestamp'])\n",
    "df_submission_frequency = pd.read_csv(directory + 'submission_frequency.csv')\n",
    "df_train = pd.read_csv(directory + 'train.csv', parse_dates=['Timestamp'])\n",
    "df_weather = pd.read_csv(directory + 'weather.csv', parse_dates=['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_holidays drop unnecessary column\n",
    "df_holidays = df_holidays.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_metadata cast dtypes as int\n",
    "df_metadata['MondayIsDayOff'] = df_metadata['MondayIsDayOff'].astype(int)\n",
    "df_metadata['TuesdayIsDayOff'] = df_metadata['TuesdayIsDayOff'].astype(int)\n",
    "df_metadata['WednesdayIsDayOff'] = df_metadata['WednesdayIsDayOff'].astype(int)\n",
    "df_metadata['ThursdayIsDayOff'] = df_metadata['ThursdayIsDayOff'].astype(int)\n",
    "df_metadata['FridayIsDayOff'] = df_metadata['FridayIsDayOff'].astype(int)\n",
    "df_metadata['SaturdayIsDayOff'] = df_metadata['SaturdayIsDayOff'].astype(int)\n",
    "df_metadata['SundayIsDayOff'] = df_metadata['SundayIsDayOff'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are three unique forecast time periods: 1440, 60, and 15 minutes\n",
    "df_submission_frequency['ForecastPeriodMin'] = (df_submission_frequency['ForecastPeriodNS']\n",
    "                                                .apply(lambda x: int(x / 60000000000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_weather drop unnecessary column\n",
    "df_weather = df_weather.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build ML DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps to Creating ML DataFrame**\n",
    "\n",
    "- Training data\n",
    "    - Account for NaN (Interpolation)\n",
    "- Merge\n",
    "    - Meta data\n",
    "    - Holidays\n",
    "    - Weather\n",
    "- Feature Engineering\n",
    "    - Categorical Feature(s) Get Dummies for Holidays\n",
    "    - Lagged features: t-1..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [D] Create DataFrame\n",
    "\n",
    "* FROM df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_1 = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of df_n_1\n",
    "shapes = collections.OrderedDict()\n",
    "shapes['df_n_1'] = df_n_1.shape\n",
    "for k,v in shapes.items(): \n",
    "    print(\"{}: \\t {}\".format(k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort values, inspect... see NaN requires interpolation\n",
    "df_n_1 = df_n_1.sort_values(['SiteId', 'ForecastId', 'Timestamp'], axis=0, ascending=[True,True,True])\n",
    "df_n_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6,974 unique ForecastId's\n",
    "df_n_1['ForecastId'].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 86,601 number of NaN values in training data set before interpolation\n",
    "df_n_1['Value'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 226 Number of SiteId with one or more single NaN values before interpolation\n",
    "df_n_1[df_n_1['Value'].isnull()].groupby(['SiteId']).sum()['Value'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2,227 Number of ForecastId with one or more single NaN values before interpolation\n",
    "df_n_1[df_n_1['Value'].isnull()].groupby(['ForecastId']).sum()['Value'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90; 360; 964: unique number of data points per ForecastId in training data\n",
    "df_n_1.groupby(['ForecastId'], as_index=False).agg({'Timestamp':'count'})['Timestamp'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [D] Add ForecastPeriodMin\n",
    "\n",
    "* FROM df_submission_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_2 = df_n_1.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_2 = df_n_2.merge(df_submission_frequency, on='ForecastId', how='inner')\n",
    "df_n_2 = df_n_2.drop(['ForecastPeriodNS'], axis=1)\n",
    "df_n_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of df_n_2\n",
    "shapes['df_n_2'] = df_n_2.shape\n",
    "for k,v in shapes.items(): \n",
    "    print(\"{}: \\t {}\".format(k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ForecastPeriodMin value_counts by submission frequency\n",
    "df_n_2['ForecastPeriodMin'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [D] Add df_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_3 = df_n_2.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Surface and BaseTemperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge Surface and BaseTemperature to ml dataframe\n",
    "df_n_3 = df_n_3.merge(df_metadata[['SiteId', 'Surface', 'BaseTemperature']], on='SiteId', how='left')\n",
    "df_n_3.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add isDayOff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Weekday to ml dataframe \n",
    "# first step is to create a dictionary of isDayOff using Weekday\n",
    "df_n_3['Weekday'] = df_n_3['Timestamp'].dt.weekday\n",
    "df_n_3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict_metadata for efficient memory and search speed when applying isDayOff to dataframe\n",
    "dict_metadata = {}\n",
    "\n",
    "for index, row in df_metadata.iterrows():\n",
    "    sid = row['SiteId'].astype(int)\n",
    "    dict_metadata[sid, 0] = row['MondayIsDayOff'].astype(int)\n",
    "    dict_metadata[sid, 1] = row['TuesdayIsDayOff'].astype(int)\n",
    "    dict_metadata[sid, 2] = row['WednesdayIsDayOff'].astype(int)\n",
    "    dict_metadata[sid, 3] = row['ThursdayIsDayOff'].astype(int)\n",
    "    dict_metadata[sid, 4] = row['FridayIsDayOff'].astype(int)\n",
    "    dict_metadata[sid, 5] = row['SaturdayIsDayOff'].astype(int)\n",
    "    dict_metadata[sid, 6] = row['SundayIsDayOff'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add isDayOff column to ml dataframe via apply lambda function referencing dict_metadata\n",
    "df_n_3['isDayOff'] = df_n_3.apply(lambda row: dict_metadata[(row['SiteId'],row['Weekday'])], axis=1)\n",
    "df_n_3 = df_n_3.drop('Weekday', axis=1)\n",
    "df_n_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of df_n_3\n",
    "shapes['df_n_3'] = df_n_3.shape\n",
    "for k,v in shapes.items(): \n",
    "    print(\"{}: \\t {}\".format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [D] Add df_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_4 = df_n_3.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a date column (without time) to merge on with df_holidays\n",
    "df_n_4['Date'] = df_n_4['Timestamp'].apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df_holidays_dates with boolean isHoliday for merging\n",
    "df_holidays_dates = df_holidays[['SiteId', 'Date']].drop_duplicates()\n",
    "df_holidays_dates['isHoliday'] = 1\n",
    "df_holidays_dates['Date'] = df_holidays_dates['Date'].apply(lambda x: x.date())\n",
    "df_holidays_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df_holidays_dates with dataframe\n",
    "df_n_4 = df_n_4.merge(df_holidays_dates, on=['SiteId', 'Date'], how='left')\n",
    "df_n_4 = df_n_4.drop('Date', axis=1)\n",
    "df_n_4.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna with 0... then make isHoliday an integer dtype\n",
    "df_n_4['isHoliday'] = df_n_4['isHoliday'].fillna(value=0)\n",
    "df_n_4['isHoliday'] = df_n_4['isHoliday'].astype(int)\n",
    "df_n_4.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"percent of days are Holidays: {}\".format(round(df_n_4['isHoliday'].sum() / df_n_4['isHoliday'].count(),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of df_n_4\n",
    "shapes['df_n_4'] = df_n_4.shape\n",
    "for k,v in shapes.items(): \n",
    "    print(\"{}: \\t {}\".format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add df_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_5 = df_n_4.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect df_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect weather data\n",
    "# see multiple Temperature for single SiteId & Timestamp combination\n",
    "# need a single SiteId & Timestamp combination to merge with ml dataframe\n",
    "df_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of weather data\n",
    "df_weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extraneous SiteIds from df_weather not found in df_train\n",
    "# shape of df_weather\n",
    "df_weather = df_weather[df_weather['SiteId'].isin(pd.Series(df_n_5['SiteId'].unique()))]\n",
    "df_weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all SiteIds in weather data are also found in the train data\n",
    "# 52 fewer unique SiteIds in both weather data & train data than in train data alone (215 vs. 267)\n",
    "# later, put these 52 SiteIds without weather data in separate ML DataFrame\n",
    "df_weather['SiteId'].unique().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep Nearest Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: view first 5 datapoints for SiteId 1\n",
    "# see multiple Temperature for single SiteId & Timestamp combination\n",
    "# need a single SiteId & Timestamp combination to merge with ml dataframe\n",
    "df_weather[(df_weather['SiteId']==1)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of nearest temperatures by SiteId and Timestamp\n",
    "# inspect df_weather_nearest to compare above and see we took the nearest Temperature\n",
    "df_weather_nearest = (df_weather.sort_values(['SiteId', 'Timestamp', 'Distance'])\n",
    "                      .groupby(['SiteId', 'Timestamp'], as_index=False).first())\n",
    "df_weather_nearest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_weather_nearest shape\n",
    "df_weather_nearest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if ther are NaN in Temperature... good, there are 0 NaN\n",
    "df_weather_nearest.Temperature.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample Weather by 15min Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_index_resample(df, time_bins='15T'):\n",
    "\n",
    "    df_list = []\n",
    "    \n",
    "    for i in df.index.unique():\n",
    "        df_rs = df[df.index == i]\n",
    "        df_rs = df_rs.reset_index().drop('SiteId', axis=1).set_index('Timestamp')\n",
    "        df_rs = df_rs.resample(time_bins).interpolate(method='linear')\n",
    "        df_rs = df_rs.reset_index()\n",
    "        df_rs['SiteId'] = i\n",
    "        df_list.append(df_rs)\n",
    "    \n",
    "    df_return = pd.concat(df_list, ignore_index=True)\n",
    "    df_return = df_return[['SiteId', 'Timestamp', 'Temperature']]\n",
    "\n",
    "    return df_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call multi_index_resample on df_weather_nearest\n",
    "mask = ['SiteId', 'Timestamp', 'Temperature']\n",
    "df_weather_nearest_resample15 = multi_index_resample(df_weather_nearest[mask].set_index('SiteId'))\n",
    "df_weather_nearest_resample15.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirmed: 215 SiteIds in resampled weather data\n",
    "df_weather_nearest_resample15.SiteId.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of df_weather_nearest_resample15\n",
    "df_weather_nearest_resample15.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Resampled Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge average temperature by SiteId and Timestamp with dataframe\n",
    "df_n_5 = df_n_5.merge(df_weather_nearest_resample15, on=['SiteId', 'Timestamp'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect dataframe\n",
    "df_n_5.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of df_n_5\n",
    "shapes['df_n_5'] = df_n_5.shape\n",
    "for k,v in shapes.items(): \n",
    "    print(\"{}: \\t {}\".format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect NaN temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1,708,262 data points are missing temperature values\n",
    "df_n_5['Temperature'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associated with 91 SiteId having 1 or more NaN Temperature values in training data\n",
    "SiteId_with_null_temps = df_n_5[df_n_5['Temperature'].isnull()]['SiteId'].unique()\n",
    "SiteId_with_null_temps.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associated with 1,832 ForecastId having 1 or more NaN Temperature values in training data\n",
    "ForecastId_with_null_temps = df_n_5[df_n_5['Temperature'].isnull()]['ForecastId'].unique()\n",
    "ForecastId_with_null_temps.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect where dataframe temperature is null (i.e. no df_weather corresponding to df_train data)\n",
    "# visual inspection shows most values for these SiteId's are NaN (where NaN=1)\n",
    "# weather will not be a useable feature for these SiteIds... drop  \n",
    "nrows = SiteId_with_null_temps.size\n",
    "ncols = 1\n",
    "\n",
    "#fig, ax = plt.subplots(ncols=ncols, nrows=nrows, sharex=True, sharey=True, figsize=(12,256), dpi=80, facecolor='w', edgecolor='k')\n",
    "#for i in range(nrows):\n",
    "#    sid = SiteId_with_null_temps[i]\n",
    "#    section = df_n_5[df_n_5['SiteId']==sid]\n",
    "#    ax[i].plot(section['Timestamp'], section['Temperature'].isnull().astype(int))\n",
    "#    ax[i].set_title(\"SiteId: {}\".format(sid));\n",
    "\n",
    "#plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now by calculations show % NaN weather values by SiteId\n",
    "df_SiteId_with_null_temps = df_n_5[df_n_5['SiteId'].isin(SiteId_with_null_temps)].copy()\n",
    "df_SiteId_with_null_temps['isNull_Temp'] = df_SiteId_with_null_temps['Temperature'].isnull()\n",
    "df_null_temps = df_SiteId_with_null_temps.groupby('SiteId').agg({'isNull_Temp':['sum', 'count']})\n",
    "df_null_temps['isNull_Temp', 'Percent_Null'] = (df_null_temps['isNull_Temp', 'sum'] \n",
    "                                                / df_null_temps['isNull_Temp', 'count'])\n",
    "df_null_temps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows that 79 of 91 SiteId's have 100% NaN weather values\n",
    "# later, let's put these 79 SiteId's + 6 with high %_Null in separate Machine Learning dataframe\n",
    "# inspect SiteId 180 & 93 for ForecastId's with weather data\n",
    "# the bottom 6 SiteId's seems reasonable to keep because the % NaN < 5% which is low\n",
    "# we can impute those values\n",
    "df_null_temps.sort_values([('isNull_Temp', 'Percent_Null')], ascending=False).tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 85 SiteId's with no weather data\n",
    "keep_SiteId = list(df_null_temps[df_null_temps[('isNull_Temp', 'Percent_Null')] <= 0.05].index)\n",
    "no_weather_SiteId = [s for s in SiteId_with_null_temps if s not in keep_SiteId]\n",
    "\n",
    "len(no_weather_SiteId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_6 = df_n_5.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect NaN Values (before interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe to inspect the distribution of NaN Value across ForecastId\n",
    "Nulls = df_n_6.groupby(['ForecastId']).apply(lambda row: row['Value'].isnull().sum())\n",
    "Entries = df_n_6.groupby(['ForecastId']).apply(lambda row: row['Timestamp'].count())\n",
    "df_NaN = pd.concat([Nulls, Entries], axis=1)\n",
    "df_NaN.columns = ['Nulls', 'Entries']\n",
    "df_NaN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with df_submission_frequency to chart by submission frequency\n",
    "df_NaN = df_NaN.merge(df_submission_frequency, left_index=True, right_on='ForecastId')\n",
    "df_NaN = df_NaN.drop('ForecastPeriodNS', axis=1)\n",
    "df_NaN = df_NaN[['ForecastId', 'ForecastPeriodMin', 'Nulls', 'Entries']]\n",
    "df_NaN['percent_NaN'] = df_NaN['Nulls'] / df_NaN['Entries']\n",
    "df_NaN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NaN['percent_NaN'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NaN.sort_values('percent_NaN', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we just delete all NaN values, we delete rows 86,601 rows (~1% of data)\n",
    "print(\"Potential to Remove \\t Rows: {} \\t Percent of Data: {}\".format(df_NaN['Nulls'].sum(), round(df_NaN['Nulls'].sum() / df_NaN['Entries'].sum(), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we delete all Forecasts that contain one or more NaN Values, we delete 2,081,362 rows (~30% data)\n",
    "print(\"Potential to Remove \\t Rows: {} \\t Percent of Data: {}\".format(df_NaN[df_NaN['percent_NaN'] > 0].Entries.sum(), round(df_NaN[df_NaN['percent_NaN'] > 0].Entries.sum() / df_NaN['Entries'].sum(), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of ForecastId's with NaN values greater than 1% of series data\n",
    "# takeaway: a lot of ForecastId's have a < 5% of missing data, while a few have more than 5%\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.hist(df_NaN[df_NaN['percent_NaN'] > 0.01]['percent_NaN'], bins=100)\n",
    "plt.title(\"Train.csv: Histogram of Percent NaN by ForecastId\", size=12)\n",
    "plt.xlabel(\"Percent NaN Value Data\", size=12)\n",
    "plt.ylabel(\"Number of ForecastId's\", size=12)\n",
    "plt.tight_layout();\n",
    "fig.savefig(\"EDA_hist_percent_NaNbyForecastId_greaterthan1percent.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [D] Interpolate NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VALIDATED - BUT ONLY WORKS MOVING FORWARD, NOT BACKFILL ###\n",
    "### https://github.com/pandas-dev/pandas/issues/10420 ###\n",
    "\n",
    "# add flag for \"is_int\"\n",
    "df_n_6['Value_Int'] = df_n_6.groupby(['SiteId', 'ForecastId'])['Value'].apply(lambda x: x.interpolate(method='linear'))\n",
    "df_n_6.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25,480 number of NaN values in training data set after interpolation\n",
    "df_n_6.Value_Int.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 58 Number of SiteId with one or more single NaN values after interpolation\n",
    "df_n_6[df_n_6.Value_Int.isnull()].groupby(['SiteId']).sum()['Value_Int'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 104 Number of ForecastId with one or more single NaN values after interpolation\n",
    "df_n_6[df_n_6['Value_Int'].isnull()].groupby(['ForecastId']).sum()['Value_Int'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect NaN Values (after interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe to inspect the distribution of NaN Value across ForecastId\n",
    "Nulls_2 = df_n_6.groupby(['ForecastId']).apply(lambda row: row['Value_Int'].isnull().sum())\n",
    "Entries_2 = df_n_6.groupby(['ForecastId']).apply(lambda row: row['Timestamp'].count())\n",
    "df_NaN_2 = pd.concat([Nulls_2, Entries_2], axis=1)\n",
    "df_NaN_2.columns = ['Nulls', 'Entries']\n",
    "df_NaN_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with df_submission_frequency to chart by frequencies\n",
    "df_NaN_2 = df_NaN_2.merge(df_submission_frequency, left_index=True, right_on='ForecastId')\n",
    "df_NaN_2 = df_NaN_2.drop('ForecastPeriodNS', axis=1)\n",
    "df_NaN_2 = df_NaN_2[['ForecastId', 'ForecastPeriodMin', 'Nulls', 'Entries']]\n",
    "df_NaN_2['percent_NaN'] = df_NaN_2['Nulls'] / df_NaN_2['Entries']\n",
    "df_NaN_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NaN_2['percent_NaN'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NaN_2.sort_values('percent_NaN', ascending=False).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the case of ForecastId == 1378, 2 NaN Value_Int remain after interpolation\n",
    "df_n_6[df_n_6['ForecastId'] == 1378].loc[:,'Value_Int'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the case of ForecastId == 1378, these NaN Value_Int are located at beginning of data\n",
    "# this is due to the linear interpolation algorithm which is range bound by data\n",
    "df_n_6[df_n_6['ForecastId'] == 1378].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex: ForecastId 608 starts 12/28 but NaN through 01/04 06\n",
    "\n",
    "x1 = df_n_6[df_n_6['ForecastId'] == 608]['Timestamp']\n",
    "y1 = df_n_6[df_n_6['ForecastId'] == 608]['Value']\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(x1, y1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of ForecastId's with NaN values greater than 1% of series data\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.hist(df_NaN_2[df_NaN_2['percent_NaN'] > 0.01]['percent_NaN'], bins=100)\n",
    "plt.title(\"Train.csv: Histogram of Percent NaN by ForecastId\", size=12)\n",
    "plt.xlabel(\"Percent NaN Value Data\", size=12)\n",
    "plt.ylabel(\"Number of ForecastId's\", size=12)\n",
    "plt.tight_layout();\n",
    "fig.savefig(\"EDA_hist_percent_NaNbyForecastId_2_greaterthan1percent.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we just delete all NaN values, we delete rows 25,480 rows (~0% of data)\n",
    "print(\"Potential to Remove \\t Rows: {} \\t Percent of Data: {}\".format(df_NaN_2['Nulls'].sum(), round(df_NaN_2['Nulls'].sum() / df_NaN_2['Entries'].sum(), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we delete all Forecasts that contain one or more NaN Values, we delete 96,426 rows (~1% data)\n",
    "print(\"Potential to Remove \\t Rows: {} \\t Percent of Data: {}\".format(df_NaN_2[df_NaN_2['percent_NaN'] > 0].Entries.sum(), round(df_NaN_2[df_NaN_2['percent_NaN'] > 0].Entries.sum() / df_NaN_2['Entries'].sum(), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [D] Add Lagged Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_7 = df_n_6.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lagged feature(s)\n",
    "df_n_7['Value_Lag_1'] = df_n_7.groupby(['SiteId', 'ForecastId'])['Value_Int'].shift(1)\n",
    "df_n_7['Value_Lag_2'] = df_n_7.groupby(['SiteId', 'ForecastId'])['Value_Int'].shift(2)\n",
    "df_n_7['Value_Lag_3'] = df_n_7.groupby(['SiteId', 'ForecastId'])['Value_Int'].shift(3)\n",
    "df_n_7['Value_Lag_4'] = df_n_7.groupby(['SiteId', 'ForecastId'])['Value_Int'].shift(4)\n",
    "df_n_7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of df_n_7\n",
    "shapes['df_n_7'] = df_n_7.shape\n",
    "for k,v in shapes.items(): \n",
    "    print(\"{}: \\t {}\".format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_8 = df_n_7.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_8.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add difference of (outside) Temperature to BaseTemperature\n",
    "df_n_8['Temp_Minus_BaseTemp'] = df_n_8['Temperature'] - df_n_8['BaseTemperature']\n",
    "df_n_8['Temp_Div_BaseTemp'] = df_n_8['Temperature'] / df_n_8['BaseTemperature']\n",
    "df_n_8.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Month & Quarter Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add month features to ml dataframe\n",
    "df_n_8['Month'] = df_n_8['Timestamp'].dt.month\n",
    "df_n_8[['Month_1', 'Month_2', 'Month_3','Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8', \\\n",
    "        'Month_9', 'Month_10', 'Month_11', 'Month_12']] = pd.get_dummies(df_n_8['Month'])\n",
    "df_n_8 = df_n_8.drop('Month', axis=1)\n",
    "df_n_8.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add quarter features to ml dataframe\n",
    "df_n_8['Quarter'] = df_n_8['Timestamp'].dt.quarter\n",
    "df_n_8[['Quarter_1', 'Quarter_2', 'Quarter_3','Quarter_4']] = pd.get_dummies(df_n_8['Quarter'])\n",
    "df_n_8 = df_n_8.drop('Quarter', axis=1)\n",
    "df_n_8.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle ML DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_8.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of df_n_8\n",
    "shapes['df_n_8'] = df_n_8.shape\n",
    "for k,v in shapes.items(): \n",
    "    print(\"{}: \\t {}\".format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_8.to_csv(directory + 'df_n_8.csv')\n",
    "\n",
    "# df_n_8.to_pickle('df_n_8.pkl')\n",
    "#df_n_7[~df_n_7['SiteId'].isin(no_weather_SiteId)].to_pickle('mldataframe.pkl')\n",
    "#df_n_7[df_n_7['SiteId'].isin(no_weather_SiteId)].to_pickle('mldataframe_noweather.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_9 = df_n_8.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sparse_n_9 = pd.SparseDataFrame(df_n_9)\n",
    "#df_sparse_n_9.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SiteId Sparse Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sid_dummies = pd.get_dummies(df_n_9['SiteId'])\n",
    "df_sid_dummies.shape\n",
    "\n",
    "#df_sparse_sid_dummies = pd.SparseDataFrame(df_sid_dummies)\n",
    "#df_sparse_sid_dummies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = []\n",
    "\n",
    "for c_name in df_sid_dummies.columns:\n",
    "    col_names.append(\"SiteId_\" + str(c_name))\n",
    "\n",
    "df_sid_dummies.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_9 = df_n_9.merge(df_sid_dummies, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time of Day Sparse Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sparse_n_9['HourMin'] = df_sparse_n_9['Timestamp'].dt.hour.astype(str) + \"_\" + df_sparse_n_9['Timestamp'].dt.minute.astype(str)\n",
    "df_n_9['HourMin'] = df_n_9['Timestamp'].dt.hour.astype(str) + \"_\" + df_n_9['Timestamp'].dt.minute.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hm_dummies = pd.get_dummies(df_n_9['HourMin'])\n",
    "df_hm_dummies.shape\n",
    "\n",
    "# df_sparse_hm_dummies = pd.SparseDataFrame(df_hm_dummies)\n",
    "# df_sparse_hm_dummies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = []\n",
    "\n",
    "for c_name in df_hm_dummies.columns:\n",
    "    col_names.append(\"HourMin_\" + str(c_name))\n",
    "\n",
    "df_hm_dummies.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some data is offset by 1min, starting at 00hr:01min instead of 00hr:00min\n",
    "for c in col_names:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_9 = df_n_9.merge(df_hm_dummies, left_index=True, right_index=True, how='left')\n",
    "df_n_9.shape\n",
    "\n",
    "# df_sparse_n_9 = df_sparse_n_9.merge(df_sparse_hm_dummies, left_index=True, right_index=True, how='left')\n",
    "# df_sparse_n_9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_9.head(2)\n",
    "# df_sparse_n_9.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle Sparse DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sparse_n_9 = pd.SparseDataFrame(df_n_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_9.to_csv(directory + 'df_n_9.csv')\n",
    "#df_n_9.to_pickle('df_n_9.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Submission Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Forecasts\n",
    "\n",
    "* Note: where ML forecast is incomplete fill-in forecast values with the best available Baseline Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict average 15min energy consumption by ForecastId\n",
    "df_submission_format.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_format.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission_1\n",
    "\n",
    "* forecast a single average value of training data for each ForecastId\n",
    "* fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_avg_forecastid = (df_train.groupby(['ForecastId'], as_index=False).agg({'Value':'mean'}))\n",
    "df_train_avg_forecastid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_1 = df_submission_format.merge(df_train_avg_forecastid, on='ForecastId', how='left')\n",
    "df_submission_1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_1 = df_submission_1.drop('Value_x', axis=1)\n",
    "df_submission_1 = df_submission_1.rename(columns={'Value_y':'Value'})\n",
    "df_submission_1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1,920 NaN Values\n",
    "df_submission_1['Value'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna with 0\n",
    "df_submission_1['Value'] = df_submission_1['Value'].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 NaN Values\n",
    "df_submission_1.Value.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index to obs_id to match submission format\n",
    "df_submission_1 = df_submission_1.set_index('obs_id')\n",
    "df_submission_1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission_1.csv\n",
    "# drivendata : 0.007459\n",
    "# df_submission_1.to_csv('submission_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission_2\n",
    "\n",
    "* forecast the average hour:minute energy consumption for each ForecastId \n",
    "    * (i.e. 96 points per 15min interval submission frequency)\n",
    "    * (i.e. 24 points per 60min interval submission frequency)\n",
    "    * (i.e.  1 point per 1140min interval submission frequency (same result as submission_1))\n",
    "* fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df_train_2 with hour:min data\n",
    "df_train_2 = df_train.copy(deep=True)\n",
    "df_train_2['Hour'] = df_train_2['Timestamp'].dt.hour\n",
    "df_train_2['Minute'] = df_train_2['Timestamp'].dt.minute\n",
    "df_train_2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average value per time submission frequency by ForecastId\n",
    "df_train_avg_forecastid_hourmin = (df_train_2.groupby(['ForecastId','Hour','Minute'], as_index=False).agg({'Value':'mean'}))\n",
    "df_train_avg_forecastid_hourmin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_avg_forecastid_hourmin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df_submission_format_2 with hour:min data\n",
    "df_submission_format_2 = df_submission_format.copy(deep=True)\n",
    "df_submission_format_2['Timestamp'] = pd.to_datetime(df_submission_format_2['Timestamp'])\n",
    "df_submission_format_2['Hour'] = df_submission_format_2['Timestamp'].dt.hour\n",
    "df_submission_format_2['Minute'] = df_submission_format_2['Timestamp'].dt.minute\n",
    "df_submission_format_2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_2 = df_submission_format_2.merge(df_train_avg_forecastid_hourmin, on=['ForecastId','Hour','Minute'], how='left')\n",
    "df_submission_2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_2 = df_submission_2.drop(['Value_x', 'Hour', 'Minute'], axis=1)\n",
    "df_submission_2 = df_submission_2.rename(columns={'Value_y':'Value'})\n",
    "df_submission_2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2,482 NaN Values\n",
    "df_submission_2['Value'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna with 0\n",
    "df_submission_2['Value'] = df_submission_2['Value'].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 NaN Values\n",
    "df_submission_2['Value'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index to obs_id to match submission format\n",
    "df_submission_2 = df_submission_2.set_index('obs_id')\n",
    "df_submission_2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission_2.csv\n",
    "# drivendata score: 0.005652\n",
    "# df_submission_2.to_csv('submission_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_submission_3\n",
    "\n",
    "* create a distribution of daily energy consumption for various SiteId's\n",
    "* make two distributions: 1. non-holiday / non-off-day, 2. holiday or off-day\n",
    "* then add a multiplier to the distributions based upon the season of year\n",
    "    * (i.e. 96 points per 15min interval submission frequency)\n",
    "    * (i.e. 24 points per 60min interval submission frequency)\n",
    "    * (i.e.  1 point per 1140min interval submission frequency (same result as submission_1))\n",
    "* fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_8.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df_n_submission_3 with hour:min data\n",
    "df_n_submission_3 = df_n_8.copy(deep=True)\n",
    "df_n_submission_3['Hour'] = df_n_submission_3['Timestamp'].dt.hour\n",
    "df_n_submission_3['Minute'] = df_n_submission_3['Timestamp'].dt.minute\n",
    "df_n_submission_3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['Value', 'Value_Lag_1', 'Value_Lag_2', 'Value_Lag_3', 'Value_Lag_4', 'Temp_Minus_BaseTemp',]\n",
    "df_n_submission_3 = df_n_submission_3.drop(drop_columns, axis=1)\n",
    "df_n_submission_3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_submission_3['isDayOffOrHoliday'] = (df_n_submission_3['isDayOff'] + df_n_submission_3['isHoliday']) > 0\n",
    "df_n_submission_3['isDayOffOrHoliday'] = df_n_submission_3['isDayOffOrHoliday'].astype(int)\n",
    "df_n_submission_3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average value per time submission frequency by ForecastId\n",
    "mask = (df_n_submission_3['ForecastPeriodMin'] == 15)\n",
    "df_s_3_split_avg_forecastid_hourmin_15 = (df_n_submission_3[mask].groupby(['SiteId','isDayOff','Hour','Minute'], as_index=False).agg({'Value_Int':'mean'}))\n",
    "df_s_3_split_avg_forecastid_hourmin_15.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the SiteId that have ForecastPeriodMin == 15\n",
    "sid_fpm_15 = df_s_3_split_avg_forecastid_hourmin_15['SiteId'].unique()\n",
    "sid_fpm_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = 41\n",
    "mask = (df_s_3_split_avg_forecastid_hourmin['SiteId'] == sid) & (df_s_3_split_avg_forecastid_hourmin['isDayOff'] == 0)\n",
    "df_s_3_split_avg_forecastid_hourmin[mask]['Value_Int'].plot()\n",
    "\n",
    "mask = (df_s_3_split_avg_forecastid_hourmin['SiteId'] == sid) & (df_s_3_split_avg_forecastid_hourmin['isDayOff'] == 1)\n",
    "df_s_3_split_avg_forecastid_hourmin[mask]['Value_Int'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "* timeseries tutorial analytics vidhya\n",
    "* start the presentation: create appendix EDA slides\n",
    "\n",
    "**Today's Goals**\n",
    "* Time Series as Index\n",
    "* Fill NaN / Ignore in train data\n",
    "* Understand train vs. submission time periods\n",
    "    * Use Forecast values as inputs for future forecasts\n",
    "* feature engineering for temp v. surface area v. ppl in building (stand in is vacation days)\n",
    "* \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
